{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile train_script.py\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport os\nimport torch.distributed as dist\ndef init_dist():\n    rank = int(os.environ[\"RANK\"])\n    world_size = int(os.environ[\"WORLD_SIZE\"])\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    if torch.cuda.is_available():\n        device=torch.device(f\"cuda:{local_rank}\")\n        dist.init_process_group(\n            backend=\"nccl\", rank=rank, world_size=world_size, device_id=device\n        )\n    else:\n        device= torch.device(\"cpu\")\n        dist.init_process_group(\n            backend=\"gloo\", rank=rank, world_size=world_size, device_id=device\n        )\n    return rank, world_size, device\n\n\n\nclass PipelineComms:\n    def __init__(self, rank, world_size):\n        self.rank = rank\n        self.world_size = world_size\n        self.prev_rank = rank - 1 if rank > 0 else None\n        self.next_rank = rank + 1 if rank < world_size - 1 else None\n\n    def send_forward(self, tensor):\n        dist.send(tensor.contiguous(), dst=self.next_rank)\n\n    def recv_forward(self, shape, device, dtype=torch.float32):\n        tensor = torch.zeros(shape, dtype=dtype, device=device)\n        dist.recv(tensor, src=self.prev_rank)\n        return tensor\n\n    def send_backward(self, tensor):\n         dist.send(tensor.contiguous(), dst=self.prev_rank)\n\n    def recv_backward(self, shape, device, dtype=torch.float32):\n        tensor = torch.zeros(shape, dtype=dtype, device=device)\n        dist.recv(tensor, src=self.next_rank)\n        return tensor\n\n    def isend_forward(self, tensor):\n        return dist.isend(tensor.contiguous(), dst=self.next_rank)\n\n\n\nclass SharedMLP(nn.Module):\n    def __init__(self, dim, total_layers, rank, world_size):\n        super().__init__()\n        self.rank=rank\n        self.is_first = rank == 0\n        self.is_last = rank + 1 == world_size\n        self.world_size=world_size\n        self.lpg= total_layers// world_size\n        layer= []\n        for i in range(self.lpg):\n            layer.append(nn.Linear(dim,dim))\n            layer.append(nn.ReLU())\n        if self.is_last:\n            layer.append(nn.Linear(dim,2))\n            self.loss_fn=nn.CrossEntropyLoss()\n        self.net=nn.Sequential(*layer)\n\n    def forward(self,x,targets=None):\n        x=self.net(x)\n        if self.is_last and targets is not None:\n            return self.loss_fn(x, targets)\n        return x\n\n\n\n\ndef gpipe_pipeline_step(model, comms, batch, targets, hidden_dim, chunks, device):\n    if comms.rank == 0:\n        micro_batches = torch.chunk(batch, chunks)\n    if comms.rank == comms.world_size - 1:\n        micro_targets = targets.chunk(chunks)\n    input_buffers = []\n    output_buffers = []\n\n    for i in range(chunks):\n        if comms.rank == 0:\n            input_data = micro_batches[i]\n        else:\n            shape = (batch // chunks, hidden_dim)\n            input_data = comms.recv_forward(shape, device)\n            input_data.requires_grad = True\n\n        if comms.rank == comms.world_size - 1:\n            output = model(input_data, micro_targets[i])\n        else:\n            output = model(input_data)\n            comms.send_forward(output.detach())\n\n        input_buffers.append(input_data)\n        output_buffers.append(output) \n        \n    if comms.rank == comms.world_size - 1:\n        total_loss = torch.zeros(output.shape, device=device)\n        \n    for i in range(chunks):\n        input_data = input_buffers[i]\n        output = output_buffers[i]\n\n        if comms.rank == comms.world_size - 1:\n            loss = output / chunks\n            loss.backward()\n            total_loss += loss\n        else:\n            grad_from_next = comms.recv_backward(output.shape, device)\n            output.backward(grad_from_next)\n\n        if comms.rank != 0:\n            comms.send_backward(input_data.grad)\n\n    if comms.rank == comms.world_size - 1:\n        return total_loss\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:25:53.829249Z","iopub.execute_input":"2026-02-14T15:25:53.832505Z","iopub.status.idle":"2026-02-14T15:25:53.853602Z","shell.execute_reply.started":"2026-02-14T15:25:53.832475Z","shell.execute_reply":"2026-02-14T15:25:53.850301Z"}},"outputs":[{"name":"stdout","text":"Writing train_script.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile trainer.py\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport os,time\nimport torch.distributed as dist\nfrom train_script import PipelineComms,init_dist,SharedMLP,gpipe_pipeline_step\n\nBATCH_SIZE = 96\nHIDDEN_DIM = 128\nTOTAL_LAYERS = 16\nSTEPS = 50\nCHUNKS = 8\nrank, world_size, device = init_dist()\ncomms = PipelineComms(rank, world_size)\ntorch.manual_seed(42)\nif rank == 0:\n    print(f\"--- Starting Micro PP on {world_size} Processes ({device}) ---\")\n\nmodel = SharedMLP(HIDDEN_DIM, TOTAL_LAYERS, rank, world_size).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nif rank == 0:\n    fixed_input = torch.randn(BATCH_SIZE, HIDDEN_DIM).to(device)\nelse:\n    fixed_input = BATCH_SIZE\n\nif rank == world_size - 1:\n   \n    fixed_target = torch.randint(0, 2, (BATCH_SIZE,)).to(device)\nelse:\n    fixed_target = None\n\nstart_time = time.time()\nmodel.train()\nfor step in range(STEPS):\n    optimizer.zero_grad()\n    if model.is_last:\n        loss = gpipe_pipeline_step(\n            model, comms, fixed_input, fixed_target, HIDDEN_DIM,3 ,device\n        )\n    else:\n       \n        gpipe_pipeline_step(\n            model, comms, fixed_input, fixed_target, HIDDEN_DIM, 3,device\n        )\n\n    optimizer.step()\n    if rank == world_size - 1 and step % 5 == 0:\n        print(f\"Step {step:02d} | Loss: {loss.item():.6f}\")\n\n\nif rank == world_size - 1:\n    print(\"--- Training Complete ---\")\n    duration = time.time() - start_time\n    print(f\"Final Loss: {loss.item():.6f} | Time: {duration:.3f}s\")\ntorch.distributed.destroy_process_group()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:27:58.356295Z","iopub.execute_input":"2026-02-14T15:27:58.356928Z","iopub.status.idle":"2026-02-14T15:27:58.362226Z","shell.execute_reply.started":"2026-02-14T15:27:58.356893Z","shell.execute_reply":"2026-02-14T15:27:58.361619Z"}},"outputs":[{"name":"stdout","text":"Overwriting trainer.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n!torchrun --nproc_per_node=2 trainer.py\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:27:58.628229Z","iopub.execute_input":"2026-02-14T15:27:58.628473Z","iopub.status.idle":"2026-02-14T15:28:08.640182Z","shell.execute_reply.started":"2026-02-14T15:27:58.628451Z","shell.execute_reply":"2026-02-14T15:28:08.639405Z"}},"outputs":[{"name":"stdout","text":"W0214 15:28:00.307000 117 torch/distributed/run.py:774] \nW0214 15:28:00.307000 117 torch/distributed/run.py:774] *****************************************\nW0214 15:28:00.307000 117 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0214 15:28:00.307000 117 torch/distributed/run.py:774] *****************************************\n--- Starting Micro PP on 2 Processes (cuda:0) ---\nStep 00 | Loss: 0.697213\nStep 05 | Loss: 0.691673\nStep 10 | Loss: 0.691702\nStep 15 | Loss: 0.691123\nStep 20 | Loss: 0.687392\nStep 25 | Loss: 0.554777\nStep 30 | Loss: 0.374644\nStep 35 | Loss: 0.328649\nStep 40 | Loss: 0.299344\nStep 45 | Loss: 0.278601\n--- Training Complete ---\nFinal Loss: 0.260255 | Time: 0.994s\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}