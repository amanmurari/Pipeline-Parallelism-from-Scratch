{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile train_script.py\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport os\nimport torch.distributed as dist\ndef init_dist():\n    rank = int(os.environ[\"RANK\"])\n    world_size = int(os.environ[\"WORLD_SIZE\"])\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    if torch.cuda.is_available():\n        device=torch.device(f\"cuda:{local_rank}\")\n        dist.init_process_group(\n            backend=\"nccl\", rank=rank, world_size=world_size, device_id=device\n        )\n    else:\n        device= torch.device(\"cpu\")\n        dist.init_process_group(\n            backend=\"gloo\", rank=rank, world_size=world_size, device_id=device\n        )\n    return rank, world_size, device\n\n\n\nclass PipelineComms:\n    def __init__(self, rank, world_size):\n        self.rank = rank\n        self.world_size = world_size\n        self.prev_rank = rank - 1 if rank > 0 else None\n        self.next_rank = rank + 1 if rank < world_size - 1 else None\n\n    def send_forward(self, tensor):\n        dist.send(tensor.contiguous(), dst=self.next_rank)\n\n    def recv_forward(self, shape, device, dtype=torch.float32):\n        tensor = torch.zeros(shape, dtype=dtype, device=device)\n        dist.recv(tensor, src=self.prev_rank)\n        return tensor\n\n    def send_backward(self, tensor):\n         dist.send(tensor.contiguous(), dst=self.prev_rank)\n\n    def recv_backward(self, shape, device, dtype=torch.float32):\n        tensor = torch.zeros(shape, dtype=dtype, device=device)\n        dist.recv(tensor, src=self.next_rank)\n        return tensor\n\n    def isend_forward(self, tensor):\n        return dist.isend(tensor.contiguous(), dst=self.next_rank)\n\n\n\nclass SharedMLP(nn.Module):\n    def __init__(self, dim, total_layers, rank, world_size):\n        super().__init__()\n        self.rank=rank\n        self.is_first = rank == 0\n        self.is_last = rank + 1 == world_size\n        self.world_size=world_size\n        self.lpg= total_layers// world_size\n        layer= []\n        for i in range(self.lpg):\n            layer.append(nn.Linear(dim,dim))\n            layer.append(nn.ReLU())\n        if self.is_last:\n            layer.append(nn.Linear(dim,2))\n            self.loss_fn=nn.CrossEntropyLoss()\n        self.net=nn.Sequential(*layer)\n\n    def forward(self,x,targets=None):\n        x=self.net(x)\n        if self.is_last and targets is not None:\n            return self.loss_fn(x, targets)\n        return x\n\n\n\n\ndef naive_pipeline_step(\n    model, comms, batch, target, hidden_dim, device\n):\n    if comms.rank == 0:\n        input_data = batch\n    else:\n        shape = (batch, hidden_dim)\n        input_data = comms.recv_forward(shape, device)\n        input_data.requires_grad = True\n        \n    output = model(input_data, target)\n    if not model.is_last:\n        comms.send_forward(output.detach())\n\n    if model.is_last:\n        loss = output\n        loss.backward()\n    else:\n        grad_from_next = comms.recv_backward(output.shape, device)\n        output.backward(grad_from_next)\n    grad_to_send = input_data.grad\n    if not model.is_first:\n        comms.send_backward(grad_to_send)\n    if model.is_last:\n        return loss\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:39:43.217418Z","iopub.execute_input":"2026-02-13T15:39:43.217693Z","iopub.status.idle":"2026-02-13T15:39:43.227099Z","shell.execute_reply.started":"2026-02-13T15:39:43.217660Z","shell.execute_reply":"2026-02-13T15:39:43.226536Z"}},"outputs":[{"name":"stdout","text":"Writing train_script.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile trainer.py\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport os,time\nimport torch.distributed as dist\nfrom train_script import PipelineComms,init_dist,SharedMLP,naive_pipeline_step\n\nBATCH_SIZE = 32\nHIDDEN_DIM = 128\nTOTAL_LAYERS = 16\nSTEPS = 50\nCHUNKS = 8\nrank, world_size, device = init_dist()\ncomms = PipelineComms(rank, world_size)\ntorch.manual_seed(42)\nif rank == 0:\n    print(f\"--- Starting Micro PP on {world_size} Processes ({device}) ---\")\n\nmodel = SharedMLP(HIDDEN_DIM, TOTAL_LAYERS, rank, world_size).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nif rank == 0:\n    fixed_input = torch.randn(BATCH_SIZE, HIDDEN_DIM).to(device)\nelse:\n    fixed_input = BATCH_SIZE\n\nif rank == world_size - 1:\n   \n    fixed_target = torch.randint(0, 2, (BATCH_SIZE,)).to(device)\nelse:\n    fixed_target = None\n\nstart_time = time.time()\nmodel.train()\nfor step in range(STEPS):\n    optimizer.zero_grad()\n    if model.is_last:\n        loss = naive_pipeline_step(\n            model, comms, fixed_input, fixed_target, HIDDEN_DIM, device\n        )\n    else:\n       \n        naive_pipeline_step(\n            model, comms, fixed_input, fixed_target, HIDDEN_DIM, device\n        )\n\n    optimizer.step()\n    if rank == world_size - 1 and step % 5 == 0:\n        print(f\"Step {step:02d} | Loss: {loss.item():.6f}\")\n\n\nif rank == world_size - 1:\n    print(\"--- Training Complete ---\")\n    duration = time.time() - start_time\n    print(f\"Final Loss: {loss.item():.6f} | Time: {duration:.3f}s\")\ntorch.distributed.destroy_process_group()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T15:42:27.485061Z","execution_failed":"2026-02-13T15:43:19.218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"hi\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-13T15:43:19.219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!torchrun --nproc_per_node=2 trainer.py","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-13T15:43:19.219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}